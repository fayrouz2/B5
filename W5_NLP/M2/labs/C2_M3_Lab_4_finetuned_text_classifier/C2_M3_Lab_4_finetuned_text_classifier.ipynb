{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "# Fine Tuning Pre-Trained Text Classifier Models\n",
    "\n",
    "Having explored how to build text processing pipelines and classifiers from the ground up, you are now ready to leverage more advanced and efficient techniques. Instead of training a model from scratch, which can be computationally intensive and require vast amounts of data, you'll use a pre-trained model. This approach utilizes a model that has already learned rich language patterns from enormous datasets, giving you a powerful head start through transfer learning.\n",
    "\n",
    "In this lab, you will focus on fine-tuning **DistilBERT**, a lighter and faster version of the formidable BERT model, to classify recipe titles. This process demonstrates how to adapt a general purpose language model for a specialized task. You'll also see how tools from the Hugging Face ecosystem streamline many of the manual data preparation steps, such as tokenization and padding.\n",
    "\n",
    "This lab will guide you through the following essential steps:\n",
    "\n",
    "* Loading the pre-trained DistilBERT model along with its specific tokenizer.\n",
    "* Preparing the recipe dataset using a custom `Dataset` class and an automated `DataCollatorWithPadding` for efficient batching.\n",
    "* Implementing two fine-tuning strategies: one where you update the entire model and another, more efficient method where you only train the final few layers.\n",
    "* Comparing the performance of both methods to evaluate the trade-offs between accuracy and computational cost.\n",
    "* Testing your model(s) on new, unseen recipe titles to assess its generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-8901-2345-67890abcdef1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6693cf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6e0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'B5'...\n",
      "remote: Enumerating objects: 785, done.\u001b[K\n",
      "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
      "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
      "remote: Total 785 (delta 66), reused 37 (delta 37), pack-reused 700 (from 2)\u001b[K\n",
      "Receiving objects: 100% (785/785), 35.63 MiB | 24.05 MiB/s, done.\n",
      "Resolving deltas: 100% (270/270), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/fayrouz2/B5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7bf079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/B5/W5_NLP/M2/labs/C2_M3_Lab_4_finetuned_text_classifier\n"
     ]
    }
   ],
   "source": [
    "%cd /content/B5/W5_NLP/M2/labs/C2_M3_Lab_4_finetuned_text_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6ce213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2_M3_Lab_4_finetuned_text_classifier.ipynb  helper_utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-a7b8-9012-3456-7890abcdef12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import transformers\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 99\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431dd49-014b-4bcf-b5b2-4ff9c81b18ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d583825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1NZjBHPzTrLahTaWcZ6GUH1GEYCTAODOw\n",
      "From (redirected): https://drive.google.com/uc?id=1NZjBHPzTrLahTaWcZ6GUH1GEYCTAODOw&confirm=t&uuid=e9d3b12d-f0cc-413e-80c6-b41117530c58\n",
      "To: /content/B5/W5_NLP/M2/labs/C2_M3_Lab_4_finetuned_text_classifier/recipes_fruit_veg.csv\n",
      "100%|██████████| 107M/107M [00:01<00:00, 59.9MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               name      id  minutes  \\\n",
      "0  a bit different  breakfast pizza   31490       30   \n",
      "1         all in the kitchen  chili  112140      130   \n",
      "2                alouette  potatoes   59389       45   \n",
      "3           apple a day  milk shake    5289        0   \n",
      "4          bananas 4 ice cream  pie   70971      180   \n",
      "\n",
      "                                         ingredients  \\\n",
      "0  ['prepared pizza crust', 'sausage patty', 'egg...   \n",
      "1  ['ground beef', 'yellow onions', 'diced tomato...   \n",
      "2  ['spreadable cheese with garlic and herbs', 'n...   \n",
      "3  ['milk', 'vanilla ice cream', 'frozen apple ju...   \n",
      "4  ['chocolate sandwich style cookies', 'chocolat...   \n",
      "\n",
      "                                               steps   category  \n",
      "0  ['preheat oven to 425 degrees f', 'press dough...  vegetable  \n",
      "1  ['brown ground beef in large pot', 'add choppe...  vegetable  \n",
      "2  ['place potatoes in a large pot of lightly sal...  vegetable  \n",
      "3  ['combine ingredients in blender', 'cover and ...      fruit  \n",
      "4  ['crumble cookies into a 9-inch pie plate , or...      fruit  \n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import pandas as pd\n",
    "\n",
    "file_id = '1NZjBHPzTrLahTaWcZ6GUH1GEYCTAODOw'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "# سيقوم هذا الأمر بتحميل الملف إلى جهازك أو البيئة التي تعمل عليها\n",
    "output = 'recipes_fruit_veg.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# الآن قم بقراءته باستخدام pandas\n",
    "df = pd.read_csv(output)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-b8c9-0123-4567-890abcdef123",
   "metadata": {},
   "source": [
    "## Revisiting Recipe Dataset\n",
    "\n",
    "You will re-use the recipe dataset from the previous lab. As a reminder, this is a specialized subset of the large [Food.com Recipes and Interactions](https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions), containing titles for recipes that have been clearly classified as either fruit-based or vegetable-based.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "* Load the `recipes_fruit_veg.csv` file into a pandas DataFrame.\n",
    "* Create a numerical `label` column from the text categories, mapping `'fruit'` to `0` and `'vegetable'` to `1`.\n",
    "* Extract the recipe names and numerical labels into two separate lists, `texts` and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-c9d0-1234-5678-90abcdef1234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for classification:  142915\n",
      "Fruit recipes:                     29148, 20.4 %\n",
      "Vegetable recipes:                 113767, 79.6 %\n"
     ]
    }
   ],
   "source": [
    "# Load the filtered dataset into a pandas DataFrame\n",
    "#df = pd.read_csv(\"recipes_fruit_veg.csv\")\n",
    "\n",
    "# Create the numerical 'label' column: 0 for 'fruit', 1 for 'vegetable'\n",
    "df['label'] = 1\n",
    "df.loc[df['category'] == 'fruit', 'label'] = 0\n",
    "\n",
    "# Extract the recipe names and labels into lists\n",
    "df_clean = df.dropna(subset=['name'])\n",
    "texts = df_clean['name'].tolist()\n",
    "labels = df_clean['label'].tolist()\n",
    "\n",
    "# Verify the dataset size and class distribution\n",
    "print(f\"Total samples for classification:  {len(texts)}\")\n",
    "print(f\"Fruit recipes:                     {labels.count(0)}, {round(labels.count(0)/(labels.count(0) + labels.count(1)) *100,1)} %\")\n",
    "print(f\"Vegetable recipes:                 {labels.count(1)}, {round(labels.count(1)/(labels.count(0) + labels.count(1)) *100,1)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4cf44-b22f-4afb-ad60-7d05b56a6142",
   "metadata": {},
   "source": [
    "### Previewing the `name`and `label` Columns\n",
    "\n",
    "Your data is now structured with the `name` and `label` columns.\n",
    "\n",
    "* Run the cell below to review a random sample of these training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5cd252-58b7-4373-b92b-986a3085af25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e27f3\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_e27f3_level0_col0\" class=\"col_heading level0 col0\" >name</th>\n",
       "      <th id=\"T_e27f3_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row0_col0\" class=\"data row0 col0\" >cajun  tomato  gravy</td>\n",
       "      <td id=\"T_e27f3_row0_col1\" class=\"data row0 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row1_col0\" class=\"data row1 col0\" >scallop soup</td>\n",
       "      <td id=\"T_e27f3_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row2_col0\" class=\"data row2 col0\" >chicken piccata light</td>\n",
       "      <td id=\"T_e27f3_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row3_col0\" class=\"data row3 col0\" >bombay kidney beans</td>\n",
       "      <td id=\"T_e27f3_row3_col1\" class=\"data row3 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row4_col0\" class=\"data row4 col0\" >surefire siu mai   dim sum</td>\n",
       "      <td id=\"T_e27f3_row4_col1\" class=\"data row4 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row5_col0\" class=\"data row5 col0\" >linguini alla critzos</td>\n",
       "      <td id=\"T_e27f3_row5_col1\" class=\"data row5 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row6_col0\" class=\"data row6 col0\" >zuppa toscana</td>\n",
       "      <td id=\"T_e27f3_row6_col1\" class=\"data row6 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row7_col0\" class=\"data row7 col0\" >maine blueberry cake</td>\n",
       "      <td id=\"T_e27f3_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row8_col0\" class=\"data row8 col0\" >lemon cream cheese coffee cake</td>\n",
       "      <td id=\"T_e27f3_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_e27f3_row9_col0\" class=\"data row9 col0\" >sweet   smoky salmon kabobs</td>\n",
       "      <td id=\"T_e27f3_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9198445eb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the number of random samples to display.\n",
    "num_samples = 10\n",
    "\n",
    "# Display a sample of name and label pairs.\n",
    "display(df[['name', 'label']].sample(num_samples, random_state=25).style.hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-d0e1-2345-6789-0abcdef12345",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Transformer\n",
    "\n",
    "In the previous lab, you built every part of your text classifier from scratch. The core difference in this lab is that you will replace several components you previously had to build yourself with highly optimized tools from the Hugging Face ecosystem.\n",
    "\n",
    "Specifically, you will be using the [DistilBERT](https://huggingface.co/distilbert-base-uncased) model. This involves loading two key components that are designed to work together:\n",
    "\n",
    "* **The Pre-trained Model**: This is a powerful neural network, DistilBERT, that has already learned to understand language from a massive amount of text. Its role is to provide a strong foundation of language understanding that you will adapt for your recipe classification task.\n",
    "\n",
    "* **The Tokenizer**: This is the bridge between your raw text and the model. It will translate your recipe titles into the specific numerical format the model was trained on. Each pre-trained model has its own specific tokenizer, and it is crucial to use the one that matches your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89cb5c-3c46-4a2a-be61-3ed033372701",
   "metadata": {},
   "source": [
    "* Execute the cell below to download the base DistilBERT model and tokenizer from the Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a29e54b-f761-4803-8d35-a1c11a91e835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading base model 'distilbert-base-uncased' to ./distilbert-local-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746c3327a0d442ae98d1c15b091b20aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe1da34fb87486680c79983e39d61c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9e9fcf0a7c4961b743f852ae9c763c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60829ff717d0418f9ecd502127b0c895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bc4e4534b14c3eb57e4cbd1f6f90df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "model_name=\"distilbert-base-uncased\"\n",
    "model_path=\"./distilbert-local-base\"\n",
    "\n",
    "# Ensure the model is downloaded\n",
    "helper_utils.download_bert(model_name, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f9a6c-ea33-4ef9-bc0b-3e890bdc6808",
   "metadata": {},
   "source": [
    "* Load the pre-trained transformer.\n",
    "    * `num_classes=2`: Attaches a new randomly initialized classification head with 2 output labels, preparing the model for your binary classification task.\n",
    "    \n",
    "**Note**: You will see a warning that some weights were \"newly initialized.\" This is expected. It confirms that you have successfully loaded the pre-trained DistilBERT base and attached a new, untrained classification head.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d708b3e-b4f4-4093-957d-4862f93e7500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at ./distilbert-local-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from ./distilbert-local-base and adding a new head with 2 classes.\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "bert_model, bert_tokenizer = helper_utils.load_bert(model_path, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2a3-4567-8901-cdef12345678",
   "metadata": {},
   "source": [
    "## Preparing Data for Training\n",
    "\n",
    "Now that you have your model, tokenizer, and data lists ready, the next step is to structure this data into the objects PyTorch requires for training. This process is simpler than in the previous lab because many of the manual steps you performed before, such as cleaning text with the `preprocess_text` function and building a custom `Vocabulary` class, are no longer necessary.\n",
    "\n",
    "The Hugging Face tokenizer handles this work for you. It performs the text cleaning, tokenization, and numerical conversion automatically inside the custom `Dataset` class you are about to create. You will define this `Dataset` to wrap your data and then use `DataLoaders` to create iterable batches.\n",
    "\n",
    "### `RecipeDataset` Dataset Class\n",
    "\n",
    "You will start by defining a `RecipeDataset` class, the purpose of which is to use your tokenizer to convert a single raw text sample into the required numerical tensors on the fly, right when the model needs it.\n",
    "\n",
    "* Define the `RecipeDataset` which will serve as a container for your data and manage the on the fly tokenization process.\n",
    "    * `__init__`: Initializes the dataset by storing your `texts`, `labels`, and the `tokenizer`.\n",
    "    * `__len__`: Returns the total number of samples in your dataset.\n",
    "    * `__getitem__`: This is the core method where the on the fly processing occurs. For each text sample, the single call to the `tokenizer` performs all the complex preprocessing steps you previously handled manually. It cleans the text, tokenizes it into sub words, converts tokens to numerical IDs using its built in vocabulary, and creates an attention mask. The method then combines these tensors with the correct label into a dictionary, ready for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2-a3b4-5678-9012-def123456789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RecipeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for text classification.\n",
    "\n",
    "    This Dataset class stores raw texts and their corresponding labels. It is\n",
    "    designed to work efficiently with a Hugging Face tokenizer, performing\n",
    "    tokenization on the fly for each sample when it is requested.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the RecipeDataset.\n",
    "\n",
    "        Args:\n",
    "            texts: A list of raw text strings.\n",
    "            labels: A list of integer labels corresponding to the texts.\n",
    "            tokenizer: A Hugging Face tokenizer instance for processing text.\n",
    "        \"\"\"\n",
    "        # Store the list of raw text strings.\n",
    "        self.texts = texts\n",
    "        # Store the list of integer labels.\n",
    "        self.labels = labels\n",
    "        # Store the tokenizer instance that will process the text.\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
    "        # Return the size of the dataset based on the number of texts.\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves and processes one sample from the dataset.\n",
    "\n",
    "        For a given index, this method fetches the corresponding text and label,\n",
    "        tokenizes the text, and returns a dictionary of tensors.\n",
    "\n",
    "        Args:\n",
    "            idx: The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the tokenized inputs ('input_ids',\n",
    "            'attention_mask') and the 'labels' as tensors.\n",
    "        \"\"\"\n",
    "        # Get the raw text and label for the specified index.\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text, handling tasks like cleaning, numerical conversion,\n",
    "        # and truncation. Padding is handled later by a DataCollator.\n",
    "        encoding = self.tokenizer(text, truncation=True, max_length=512)\n",
    "\n",
    "        # Add the label to the encoding dictionary and convert it to a tensor.\n",
    "        encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        # Return the dictionary containing all processed data for the sample.\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cd2f9b-3c53-45f6-b567-401e753f8c1d",
   "metadata": {},
   "source": [
    "* Create an instance of your `RecipeDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b02f7-e93a-4627-ab95-0874e0ccb6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the full dataset\n",
    "full_dataset = RecipeDataset(texts, labels, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2a3-b4c5-6789-0123-ef1234567890",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "* Divide your `full_dataset` into an 80% training set and a 20% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ceb6f9-7e94-48ce-9ad2-f0dd9ff9bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:   114332\n",
      "Validation samples: 28583\n"
     ]
    }
   ],
   "source": [
    "# Split the full dataset into an 80% training set and a 20% validation set.\n",
    "train_dataset, val_dataset = helper_utils.create_dataset_splits(\n",
    "    full_dataset, \n",
    "    train_split_percentage=0.8\n",
    ")\n",
    "\n",
    "# Print the number of samples in each set to verify the split.\n",
    "print(f\"Training samples:   {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7571826-d397-40a9-88bf-ce2d0bf7c5bb",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "In the previous lab, you addressed the challenge of batching variable-length text by writing custom `collate_fn` functions to manually pad sequences or create offsets. The Hugging Face `DataCollatorWithPadding` function automates this complex step for you. \n",
    "\n",
    "* Use [DataCollatorWithPadding](https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding) and pass it your `bert_tokenizer`. It will automatically handle the dynamic padding of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491f4cc-d195-4d30-a6e6-300b041b941f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data collator handles dynamic padding for each batch\n",
    "data_collator = transformers.DataCollatorWithPadding(tokenizer=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d851d26-3b71-4c12-b13a-2a5b92fa56fb",
   "metadata": {},
   "source": [
    "* Create two `DataLoader` instances, `train_loader` and `val_loader`.\n",
    "    * `collate_fn=data_collator`: Passing your `data_collator` to create dynamically padded batches instead of the default PyTorch behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2a3b4-c5d6-7890-1234-f12345678901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the number of samples to process in each batch.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for the training set with `data_collator`\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=data_collator\n",
    "                         )\n",
    "\n",
    "# Create the DataLoader for the validation set with `data_collator`\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        collate_fn=data_collator\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5-d6e7-8901-2345-123456789012",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "With the pre-trained DistilBERT model loaded and the `DataLoaders` fully configured, the foundational work is complete. You are now ready to begin the fine-tuning process.\n",
    "\n",
    "### Addressing Class Imbalance\n",
    "\n",
    "* Calculate class weights to address the data imbalance in your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a679256-2401-4833-bf38-278b92c2f29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Class Weights:\n",
      "  - Fruit (Class 0):     2.45\n",
      "  - Vegetable (Class 1): 0.63\n"
     ]
    }
   ],
   "source": [
    "# Extract all labels from the training set to calculate class weights for handling imbalance.\n",
    "train_labels_list = [train_dataset.dataset.labels[i] for i in train_dataset.indices]\n",
    "    \n",
    "    \n",
    "# Use scikit-learn's utility to automatically calculate class weights.\n",
    "class_weights = compute_class_weight(\n",
    "    # The strategy for calculating weights. 'balanced' is automatic.\n",
    "    class_weight='balanced',\n",
    "    # The array of unique class labels (e.g., [0, 1]).\n",
    "    classes=np.unique(train_labels_list),\n",
    "    # The list of all training labels, used to count class frequencies.\n",
    "    y=train_labels_list\n",
    ")\n",
    "\n",
    "# Convert the NumPy array of weights into a PyTorch tensor of type float\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Print the final weights to verify the calculation.\n",
    "print(\"Calculated Class Weights:\")\n",
    "print(f\"  - Fruit (Class 0):     {class_weights[0]:.2f}\")\n",
    "print(f\"  - Vegetable (Class 1): {class_weights[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4040e-7743-42c9-a291-6e57f78eb278",
   "metadata": {},
   "source": [
    "### Configuring the Loss Function\n",
    "\n",
    "* Define `nn.CrossEntropyLoss` as your loss function, and pass your previously calculated `class_weights` tensor to the weight parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986c70f-6303-45f9-a3d3-fef84093db08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the CrossEntropyLoss function with the calculated `class_weights`.\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d9e65-e090-4ccf-81cc-02ae6f35905f",
   "metadata": {},
   "source": [
    "### Baseline Approach: Fine-Tuning the Entire Model\n",
    "\n",
    "First, you will take the standard approach: fine-tuning the *entire* DistilBERT model. This means that every parameter, from the initial embedding layers to the final classification layer, will have its weights updated during training. Keep in mind that we are beginning the training using the pre-trained weights and will continue to further train the model.\n",
    "\n",
    "This method adapts the whole model to the recipe classification task and will serve as your performance baseline. You will use the `training_loop` function to run the training process and see how well this approach works.\n",
    "\n",
    "* For each batch, it explicitly unpacks the `input_ids`, `attention_mask`, and `labels` required by the model.\n",
    "* It then fine-tunes all layers of the DistilBERT model on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24bfdc-dd97-412c-b8a8-adc1e2fe6335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def training_loop(model, train_loader, val_loader, loss_function, num_epochs, device):\n",
       "    \"\"\"\n",
       "    Performs a full training and validation cycle for a PyTorch model.\n",
       "\n",
       "    Args:\n",
       "        model: The PyTorch model to be trained.\n",
       "        train_loader: The DataLoader for the training dataset.\n",
       "        val_loader: The DataLoader for the validation dataset.\n",
       "        loss_function: The loss function used for training.\n",
       "        num_epochs: The total number of epochs to train for.\n",
       "        device: The computational device ('cuda' or 'cpu') to run on.\n",
       "\n",
       "    Returns:\n",
       "        A tuple containing the trained model and a dictionary of the final\n",
       "        performance metrics from the last validation epoch.\n",
       "    \"\"\"\n",
       "    # Move the model to the specified computational device.\n",
       "    model.to(device)\n",
       "\n",
       "    # Initialize the AdamW optimizer with a default learning rate.\n",
       "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
       "\n",
       "    # Determine the number of classes from the model's configuration.\n",
       "    num_classes = model.config.num_labels\n",
       "\n",
       "    # Initialize metric objects from torchmetrics for stateful metric calculation.\n",
       "    val_accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes).to(device)\n",
       "    val_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
       "    val_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
       "    val_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
       "\n",
       "    # Create the main progress bar that iterates over the epochs.\n",
       "    epoch_loop = tqdm(range(num_epochs), desc=\"Training Progress\")\n",
       "\n",
       "    # Begin the main training and validation loop.\n",
       "    for epoch in epoch_loop:\n",
       "\n",
       "        # --- Training Phase ---\n",
       "        # Set the model to training mode, which enables layers like dropout.\n",
       "        model.train()\n",
       "        # Initialize the accumulated training loss for the epoch.\n",
       "        train_loss_epoch = 0\n",
       "\n",
       "        # Create a nested progress bar for the training batches of the current epoch.\n",
       "        train_inner_loop = tqdm(\n",
       "            train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\", leave=False\n",
       "        )\n",
       "        # Iterate over the training data batches.\n",
       "        for batch in train_inner_loop:\n",
       "            # Unpack the batch and move all tensors to the active device.\n",
       "            input_ids = batch[\"input_ids\"].to(device)\n",
       "            attention_mask = batch[\"attention_mask\"].to(device)\n",
       "            labels = batch[\"labels\"].to(device)\n",
       "\n",
       "            # Clear any gradients from the previous iteration.\n",
       "            optimizer.zero_grad()\n",
       "\n",
       "            # Perform a forward pass to get the model's raw outputs (logits).\n",
       "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
       "            logits = outputs.logits\n",
       "\n",
       "            # Calculate the loss for the current batch.\n",
       "            loss = loss_function(logits, labels)\n",
       "\n",
       "            # Accumulate the loss and perform backpropagation to compute gradients.\n",
       "            train_loss_epoch += loss.item()\n",
       "            loss.backward()\n",
       "\n",
       "            # Update the model's weights based on the computed gradients.\n",
       "            optimizer.step()\n",
       "\n",
       "            # Update the inner progress bar's postfix with the current batch loss.\n",
       "            train_inner_loop.set_postfix(loss=loss.item())\n",
       "\n",
       "        # Calculate the average training loss over all batches in the epoch.\n",
       "        train_loss_epoch /= len(train_loader)\n",
       "\n",
       "        # --- Validation Phase ---\n",
       "        # Set the model to evaluation mode, which disables layers like dropout.\n",
       "        model.eval()\n",
       "        # Initialize the accumulated validation loss for the epoch.\n",
       "        val_loss_epoch = 0\n",
       "\n",
       "        # Create a nested progress bar for the validation batches.\n",
       "        val_inner_loop = tqdm(\n",
       "            val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Validation\", leave=False\n",
       "        )\n",
       "        # Disable gradient calculations to save memory and computations.\n",
       "        with torch.no_grad():\n",
       "            # Iterate over the validation data batches.\n",
       "            for batch in val_inner_loop:\n",
       "                # Unpack the batch and move tensors to the active device.\n",
       "                input_ids = batch[\"input_ids\"].to(device)\n",
       "                attention_mask = batch[\"attention_mask\"].to(device)\n",
       "                labels = batch[\"labels\"].to(device)\n",
       "\n",
       "                # Perform a forward pass to get the model's logits.\n",
       "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
       "                logits = outputs.logits\n",
       "\n",
       "                # Calculate the validation loss for the current batch.\n",
       "                val_loss = loss_function(logits, labels)\n",
       "                val_loss_epoch += val_loss.item()\n",
       "\n",
       "                # Get model predictions and update the metric objects with batch results.\n",
       "                preds = torch.argmax(logits, dim=-1)\n",
       "                val_accuracy.update(preds, labels)\n",
       "                val_precision.update(preds, labels)\n",
       "                val_recall.update(preds, labels)\n",
       "                val_f1.update(preds, labels)\n",
       "        \n",
       "        # Calculate the average validation loss for the epoch.\n",
       "        val_loss_epoch /= len(val_loader)\n",
       "\n",
       "        # --- Logging and Metric Calculation ---\n",
       "        # Compute the final metrics over the entire validation set for the epoch.\n",
       "        epoch_acc = val_accuracy.compute()\n",
       "        epoch_prec = val_precision.compute()\n",
       "        epoch_recall = val_recall.compute()\n",
       "        epoch_f1 = val_f1.compute()\n",
       "\n",
       "        # Reset all metric objects to be ready for the next epoch.\n",
       "        val_accuracy.reset()\n",
       "        val_precision.reset()\n",
       "        val_recall.reset()\n",
       "        val_f1.reset()\n",
       "\n",
       "        # Update the main progress bar with the results of the completed epoch.\n",
       "        epoch_loop.set_postfix(\n",
       "            train_loss=f\"{train_loss_epoch:.4f}\",\n",
       "            val_loss=f\"{val_loss_epoch:.4f}\",\n",
       "            val_acc=f\"{epoch_acc:.4f}\",\n",
       "        )\n",
       "        # Use tqdm.write to log metrics without interfering with the progress bars.\n",
       "        tqdm.write(\n",
       "            f\"Epoch {epoch+1} Metrics -> Val Acc: {epoch_acc:.4f}, Val F1: {epoch_f1:.4f}\"\n",
       "        )\n",
       "\n",
       "    # Indicate that the entire training process is complete.\n",
       "    print(\"\\n--- Training complete ---\")\n",
       "\n",
       "    # Store the final metrics from the last epoch in a dictionary.\n",
       "    final_results = {\n",
       "        \"val_accuracy\": epoch_acc.item(),\n",
       "        \"val_precision\": epoch_prec.item(),\n",
       "        \"val_recall\": epoch_recall.item(),\n",
       "        \"val_f1\": epoch_f1.item(),\n",
       "    }\n",
       "    \n",
       "    # Return the trained model and the final results.\n",
       "    return model, final_results\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Uncomment if you want to see the training loop function\n",
    "helper_utils.display_function(helper_utils.training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf0069b-358c-492f-a2e1-6076edb1499f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = input): <text>:5:20: unexpected ','\n4: # Call the training loop to start the full fine-tuning process.\n5: full_finetuned_bert,\n                      ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = input): <text>:5:20: unexpected ','\n4: # Call the training loop to start the full fine-tuning process.\n5: full_finetuned_bert,\n                      ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Set the total number of epochs.\n",
    "num_epochs = 3\n",
    "\n",
    "# Call the training loop to start the full fine-tuning process.\n",
    "full_finetuned_bert, full_results = helper_utils.training_loop(\n",
    "    bert_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3ee10-4956-4d1e-b9c5-1f84ca3faf5d",
   "metadata": {},
   "source": [
    "* Print the validation metrics from the `results_bert` dictionary to review the performance of your fine-tuned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9028914-941b-44dd-90c9-5154737b395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation Metrics\n",
      "\n",
      "Accuracy:   0.9580\n",
      "Precision:  0.9351\n",
      "Recall:     0.9360\n",
      "F1:         0.9356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the results \n",
    "helper_utils.print_final_results(full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea9a30",
   "metadata": {},
   "source": [
    "### An Efficient Alternative: Partial Fine-Tuning\n",
    "\n",
    "While fine-tuning the entire model is effective, it can be computationally expensive. Now, you will explore a more efficient strategy known as **partial fine-tuning**. Instead of training the entire model, you will strategically freeze the majority of the model's layers and train only those most effective for adapting to the new task.\n",
    "\n",
    "First take a look at the architecture of the DistilBERT model you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf653b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb6392-edba-4eaf-8256-49e36011e1cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The decision of which layers to freeze is based on how transformers learn hierarchically:\n",
    "\n",
    "* **Earlier Layers**: \n",
    "The layers closer to the input learn general language features, such as grammar and basic word relationships. \n",
    "Since these features are useful for almost any task, they are often kept frozen. \n",
    "In your DistilBERT model, these are the `embeddings` and the **first four** `TransformerBlock` layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ff223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings: \n",
      "\n",
      "Embeddings(\n",
      "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "  (position_embeddings): Embedding(512, 768)\n",
      "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "First four TransformerBlock layers: \n",
      "\n",
      "ModuleList(\n",
      "  (0-3): 4 x TransformerBlock(\n",
      "    (attention): DistilBertSdpaAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (ffn): FFN(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# embeddings\n",
    "print(\"\\nEmbeddings: \\n\")\n",
    "print(bert_model.distilbert.embeddings)\n",
    "\n",
    "# first four TransformerBlock layers\n",
    "print(\"\\nFirst four TransformerBlock layers: \\n\")\n",
    "print(bert_model.distilbert.transformer.layer[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a48f28",
   "metadata": {},
   "source": [
    "* **Later Layers**: The layers closer to the output learn more complex and abstract features that become more specialized to the data they are trained on. These are the layers you typically want to unfreeze to adapt the model to the nuances of your new task. In your model, these are the **last two** `TransformerBlock` layers and the final classification layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef8017-d932-45c7-8cc6-4a18ac411750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last two TransformerBlock layers: \n",
      "\n",
      "ModuleList(\n",
      "  (0-1): 2 x TransformerBlock(\n",
      "    (attention): DistilBertSdpaAttention(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (ffn): FFN(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (activation): GELUActivation()\n",
      "    )\n",
      "    (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Final Classifier Layer: \n",
      "\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# last two TransformerBlock layers\n",
    "print(\"\\nLast two TransformerBlock layers: \\n\")\n",
    "print(bert_model.distilbert.transformer.layer[4:6])\n",
    "\n",
    "# final classification layers\n",
    "print(\"\\nFinal Classifier Layer: \\n\")\n",
    "print(bert_model.pre_classifier)\n",
    "print(bert_model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be458123",
   "metadata": {},
   "source": [
    "For the task at hand, you will unfreeze and train the **final classifier head** and the **last two transformer layers** (**later layers**). This allows the model to adjust its high level feature extraction to the nuances of recipe classification, while still leveraging the robust, general language understanding from its frozen layers.\n",
    "\n",
    "This approach tests a key hypothesis: can you achieve comparable performance to the baseline while saving significant computational resources?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e008a-6227-4c7e-8531-4bf3819de99d",
   "metadata": {},
   "source": [
    "* Your first step is to freeze all parameters in the model by setting their `requires_grad` attribute to `False`. This prevents their weights from being updated during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af696ef3-96bb-46b5-9105-23404548d2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze ALL model parameters first\n",
    "for param in bert_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80236d-7d5a-41df-8c5f-585dab144a12",
   "metadata": {},
   "source": [
    "* Next, you will unfreeze the **last two transformer layers** to make them trainable by setting their `requires_grad` attribute back to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c28eb-a782-452d-ab94-78b1dced3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the last 2 transformer layers\n",
    "# Set the number of final transformer layers to unfreeze and train.\n",
    "layers_to_train = 2 \n",
    "\n",
    "# Access the list of all transformer layers in the DistilBERT model.\n",
    "transformer_layers = bert_model.distilbert.transformer.layer\n",
    "\n",
    "# Loop backwards from the end of the layer list for the number of layers you want to train.\n",
    "for i in range(layers_to_train):\n",
    "    # Select a layer using negative indexing (e.g., -1 for the last, -2 for the second to last).\n",
    "    layer_to_unfreeze = transformer_layers[-(i+1)]\n",
    "    \n",
    "    # Iterate through all parameters of the selected layer.\n",
    "    for param in layer_to_unfreeze.parameters():\n",
    "        # Set requires_grad to True to make the parameter trainable.\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1f7ba-8c52-476f-a6b2-3670f9424089",
   "metadata": {},
   "source": [
    "* The final step is to unfreeze the model's classification head, which consists of the `pre_classifier` and `classifier` layers, to ensure it can be trained on your new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15155c0f-1f80-4dcd-92a9-a9db4c10a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the classifier head\n",
    "# The final layers of the model must be made trainable to adapt to the new task.\n",
    "\n",
    "# For DistilBERT, this head consists of two linear layers.\n",
    "# Unfreeze the pre_classifier layer.\n",
    "for param in bert_model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final classifier layer.\n",
    "for param in bert_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70356d0d-533a-4cb2-beb1-7db2369679a1",
   "metadata": {},
   "source": [
    "With your partial fine-tuning strategy configured, you will now execute the `training_loop` function to start the training process. This will handle the entire training process and return the trained model along with a dictionary of the final validation metrics.\n",
    "\n",
    "* For each batch, it explicitly unpacks the `input_ids`, `attention_mask`, and `labels` required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359a48d3-28de-43d8-a90b-e9fcbc69a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the training loop function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.training_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c8be4-15b0-41a8-b524-01ea310cdfa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8683c512b34bbebf30788a42a37ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee2b1094986415cbc03d26ec98b2a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3 Training:   0%|          | 0/3573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb12dde0f4c0438ebed695cef7587dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3 Validation:   0%|          | 0/894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Metrics -> Val Acc: 0.9523, Val F1: 0.9282\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51ba843abf3a4f78acab6afd8d0c7ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3 Training:   0%|          | 0/3573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the total number of epochs.\n",
    "num_epochs = 3\n",
    "\n",
    "# Call the training loop to start the partial fine-tuning process.\n",
    "partial_finetuned_bert, partial_results = helper_utils.training_loop(\n",
    "    bert_model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_function, \n",
    "    num_epochs, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd1f46-ef99-4c96-ba12-4aae8e40b2fe",
   "metadata": {},
   "source": [
    "* Print the validation metrics from the `results_bert` dictionary to review the performance of your fine-tuned model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee852c32-854c-4f0c-a355-8c1d46edeab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results \n",
    "helper_utils.print_final_results(partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada1cd-91c9-4176-a360-be65609fe5ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Comparing Fine-Tuning Approaches\n",
    "\n",
    "* Directly compare the performance of the two approaches: the full fine-tuning baseline and the efficient partial fine-tuning method.\n",
    "    * `full_results`: Contains the metrics from the full fine-tuning of the entire model.\n",
    "    * `partial_results`: Contains the metrics from the partial fine-tuning, where only the last two transformer layers and the classifier were trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80690400-0910-4b5c-acb7-00a495f78ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare your results\n",
    "helper_utils.display_results(full_results, partial_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4826c5-0a9a-4c40-a5e2-26e417338af5",
   "metadata": {},
   "source": [
    "Based on these results, it's clear that both models perform almost identically after just `3` epochs. As you must have noticed, this more efficient approach took less time to train since it updated far fewer parameters. This perfectly illustrates the core benefit of partial fine-tuning: achieving comparable, if not better, performance while saving valuable time and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6e7f8-a9b0-1234-5678-456789012345",
   "metadata": {},
   "source": [
    "## Testing the Fine-tuned BERT Model on New Examples\n",
    "\n",
    "Now for the final test. It's time to see how your fine-tuned model performs on completely new, unseen data. This is the best way to get a qualitative feel for how well your model has learned to generalize.\n",
    "\n",
    "* Define a `test_products` list containing a mix of new recipe titles. This list includes straightforward examples as well as more challenging ones to see where the model excels and where it might struggle.\n",
    "    * Feel free to add your own recipe titles to this list to test the model even further!\n",
    " \n",
    "**Note**: Remember, the model's predictions are based *only* on the words in the recipe's `name`. It was never shown the ingredients list, so it has no knowledge of whether fruits or vegetables are the dominant ingredient. A recipe's name can sometimes be misleading, and the model's classification will reflect only what it has learned from the title's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375825f-0e7e-4464-8a58-ac6485b65206",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_products = [\n",
    "    \"Blueberry Muffins\",                  # Expected: Fruit\n",
    "    \"Spinach and Feta Stuffed Chicken\",   # Expected: Vegetable\n",
    "    \"Classic Carrot Cake with Frosting\",  # Expected: Vegetable\n",
    "    \"Tomato and Basil Bruschetta\",        # Expected: Vegetable\n",
    "    \"Avocado Toast\",                      # Expected: Fruit\n",
    "    \"Zucchini Bread with Walnuts\",        # Expected: Vegetable\n",
    "    \"Lemon and Herb Roasted Chicken\",     # Expected: Fruit\n",
    "    \"Strawberry Rhubarb Pie\",             # Expected: Fruit\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fbe0f-e2d5-4dc6-8557-7d242efc5b3f",
   "metadata": {},
   "source": [
    "* Finally, loop through the `test_products` list to run the prediction for each recipe and see the model's final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975aa376-091d-429d-916e-54bfe71c5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment if you want to see the predict category function\n",
    "\n",
    "# helper_utils.display_function(helper_utils.predict_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7f8a9-b0c1-2345-6789-567890123456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through each test product\n",
    "for product in test_products:\n",
    "    # Call the prediction function with the required arguments\n",
    "    category = helper_utils.predict_category(\n",
    "        partial_finetuned_bert, # Try it with `full_finetuned_bert` as well.\n",
    "        bert_tokenizer,\n",
    "        product,\n",
    "        device\n",
    "    )\n",
    "    # Print the results\n",
    "    print(f\"Product: '{product}'\\nPredicted: {category}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf08ebd-51a5-44a9-948d-ebb669a98b12",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations on completing this lab! You have successfully moved beyond building models from scratch and have now fine-tuned a state-of-the-art transformer model for a custom text classification task.\n",
    "\n",
    "You began by loading a pre-trained DistilBERT model and saw firsthand how it simplifies the entire text-to-tensor pipeline. The main takeaway from your experiments is the effectiveness of **partial fine-tuning**. You demonstrated that by strategically freezing most of the model's layers and only updating the final, task-specific ones, you can achieve performance comparable to or even slightly better than fully fine-tuning the entire model. This insight is immensely valuable for practical applications, as it allows for significant savings in training time and computational resources without sacrificing quality.\n",
    "\n",
    "The skills you've developed here, loading and adapting pre-trained models, managing data with modern tools, and strategically choosing which parts of a model to train—are the building blocks for tackling a wide range of complex NLP challenges, from sentiment analysis to machine translation and beyond."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
