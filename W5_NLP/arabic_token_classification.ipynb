{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855f631",
   "metadata": {},
   "source": [
    "# Arabic Named-Entity Recognition (NER) — Assignment\n",
    "\n",
    "This notebook guides you through building an Arabic NER model using the ANERCorp dataset (`asas-ai/ANERCorp`). Fill in the TODO cells to complete the exercise.\n",
    "\n",
    "- **Objective:** Train a token-classification model (NER) that labels tokens with entity tags (e.g., people, locations, organizations).\n",
    "- **Dataset:** `asas-ai/ANERCorp` — contains tokenized Arabic text and tag sequences.\n",
    "- **Typical Labels:** `B-PER`, `I-PER` (person), `B-LOC`, `I-LOC` (location), `B-ORG`, `I-ORG` (organization), and `O` (outside/no entity). Your code should extract the exact label set from the dataset and build `label_list`, `id2label`, and `label2id` mappings.\n",
    "- **Key Steps (what you will implement):**\n",
    "  1. Load the dataset and inspect samples.\n",
    "  2. Convert the provided words into sentence groupings (use `.` `?` `!` as sentence delimiters) before tokenization so sentence boundaries are preserved.\n",
    "  3. Tokenize with a pretrained Arabic tokenizer and align tokenized sub-words with original labels (use `-100` for tokens to ignore in loss).\n",
    "  4. Prepare `tokenized_datasets` and data collator for dynamic padding.\n",
    "  5. Configure and run model training using `AutoModelForTokenClassification` and `Trainer`.\n",
    "  6. Evaluate using `seqeval` (report precision, recall, F1, and accuracy) and run inference with a pipeline.\n",
    "\n",
    "- **Evaluation:** Use the `seqeval` metric (entity-level precision, recall, F1). When aligning predictions and labels, filter out `-100` entries so only real token labels are compared.\n",
    "\n",
    "- **Deliverables:** Completed notebook with working cells for data loading, tokenization/label alignment, training, evaluation, and an inference example. Add short comments explaining choices (e.g., sentence-splitting strategy, tokenizer settings).\n",
    "\n",
    "Good luck — implement each TODO in order and run the cells to verify output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b101905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# TODO: Install the required packages for Arabic NER with transformers\n",
    "# Required packages: transformers, datasets, seqeval, evaluate, accelerate\n",
    "# Use pip install with -q flag to suppress output\n",
    "\n",
    "!pip install transformers datasets seqeval evaluate accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abb573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "# TODO: List the files in the current directory to explore the workspace\n",
    "# Hint: Use a simple command to display directory contents\n",
    "\n",
    "# YOUR CODE HERE\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da23007",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211446a8256e4ababe209d7c872caa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34df9f8eb724c57bb0a6ed61e593ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-83c5047e14e689(…):   0%|          | 0.00/855k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5040db4903f44739a6d23b5a4990969e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001-245173671c05c71(…):   0%|          | 0.00/175k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6d12e896974562a8f3c12b675f3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/125102 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295613f0458c477f808f3240837de395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25008 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 125102\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word', 'tag'],\n",
      "        num_rows: 25008\n",
      "    })\n",
      "})\n",
      "Sample Entry: {'word': ['فرانكفورت', '(د', 'ب', 'أ)', 'أعلن'], 'tag': ['B-LOC', 'O', 'O', 'O', 'O']}\n",
      "\n",
      "Label List: ['B-LOC', 'B-MISC', 'B-ORG', 'B-PERS', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PERS', 'O']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the ANERCorp dataset and extract label mappings\n",
    "# Steps:\n",
    "# 1. Import required libraries (datasets, numpy)\n",
    "# 2. Load the \"asas-ai/ANERCorp\" dataset using load_dataset()\n",
    "# 3. Inspect the dataset structure - print the splits and a sample entry\n",
    "# 4. Extract unique tags from the training split\n",
    "# 5. Create label_list (sorted), id2label, and label2id mappings\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "dataset = datasets.load_dataset(\"asas-ai/ANERCorp\")\n",
    "print(f\"Dataset Split: {dataset}\")\n",
    "print(f\"Sample Entry: {dataset['train'][0:5]}\")\n",
    "\n",
    "unique_tags = {tag for tag in dataset[\"train\"][\"tag\"]}\n",
    "\n",
    "label_list = sorted(unique_tags)\n",
    "id2label = dict(enumerate(label_list))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "print(f\"\\nLabel List: {label_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6aa5b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word     tag\n",
      "count   125102  125102\n",
      "unique   29252       9\n",
      "top          .       O\n",
      "freq      4149  111921\n",
      "-----------------------\n",
      "         word    tag\n",
      "count   25008  25008\n",
      "unique   9075      9\n",
      "top         .      O\n",
      "freq      961  21633\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify the dataset was loaded correctly\n",
    "# Print the dataframe or dataset summary to inspect the data structure\n",
    "\n",
    "# YOUR CODE HERE\n",
    "print(dataset[\"train\"].to_pandas().describe())\n",
    "print(\"-\"*23)\n",
    "print(dataset[\"test\"].to_pandas().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be78b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer and create tokenization function\n",
    "# Steps:\n",
    "# 1. Import AutoTokenizer from transformers\n",
    "# 2. Set model_checkpoint to \"aubmindlab/bert-base-arabertv02\"\n",
    "# 3. Load the tokenizer using AutoTokenizer.from_pretrained()\n",
    "# 4. Create tokenize_and_align_labels function that:\n",
    "#    - Tokenizes the input text (is_split_into_words=True)\n",
    "#    - Maps tokens to their original words\n",
    "#    - Handles special tokens by setting them to -100\n",
    "#    - Aligns labels with sub-word tokens\n",
    "#    - Returns tokenized inputs with labels\n",
    "# 5. Important: Convert words to sentences using punctuation marks \".?!\" as sentence delimiters\n",
    "#    - This helps the model understand sentence boundaries\n",
    "#    - Hint (suggested approach): group `examples['word']` into sentence lists using \".?!\" as end markers, e.g.:\n",
    "#        sentences = []\n",
    "#        current = []\n",
    "#        for w in examples['word']:\n",
    "#            current.append(w)\n",
    "#            if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "#                sentences.append(current)\n",
    "#                current = []\n",
    "#        if current:\n",
    "#            sentences.append(current)\n",
    "#      Then align `examples['tag']` accordingly to these sentence groups before tokenization.\n",
    "# 6. Apply the function to the entire dataset using dataset.map()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model_checkpoint = \"aubmindlab/bert-base-arabertv02\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def group_into_sentences(words, tags):\n",
    "    sentences, sentence_tags = [], []\n",
    "    current_words, current_tags = [], []\n",
    "    for w, t in zip(words, tags):\n",
    "        current_words.append(w)\n",
    "        current_tags.append(t)\n",
    "        if w in ['.', '?', '!'] or (len(w) > 0 and w[-1] in '.?!'):\n",
    "            sentences.append(current_words)\n",
    "            sentence_tags.append(current_tags)\n",
    "            current_words, current_tags = [], []\n",
    "    if current_words:\n",
    "        sentences.append(current_words)\n",
    "        sentence_tags.append(current_tags)\n",
    "    return sentences, sentence_tags\n",
    "\n",
    "train_sentences, train_tags = group_into_sentences(dataset['train']['word'], dataset['train']['tag'])\n",
    "test_sentences, test_tags = group_into_sentences(dataset['test']['word'], dataset['test']['tag'])\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(words_list, tags_list):\n",
    "    all_inputs = []\n",
    "    for words, tags in zip(words_list, tags_list):\n",
    "        tokenized_inputs = tokenizer(words, is_split_into_words=True, truncation=True, padding=True)\n",
    "        word_ids = tokenized_inputs.word_ids()\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(label2id[tags[word_idx]])\n",
    "            else:\n",
    "                aligned_labels.append(label2id[tags[word_idx]] if tags[word_idx].startswith(\"I-\") else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        tokenized_inputs[\"labels\"] = aligned_labels\n",
    "        all_inputs.append(tokenized_inputs)\n",
    "    return all_inputs\n",
    "\n",
    "tokenized_train = tokenize_and_align_labels(train_sentences, train_tags)\n",
    "tokenized_test = tokenize_and_align_labels(test_sentences, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e5b09f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cfa5ca394c4d538a3e7d48916d864a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Define the compute_metrics function for model evaluation\n",
    "# Steps:\n",
    "# 1. Import evaluate and load \"seqeval\" metric\n",
    "# 2. Create compute_metrics function that:\n",
    "#    - Extracts predictions from model outputs using argmax\n",
    "#    - Filters out -100 labels (special tokens and sub-words)\n",
    "#    - Converts prediction and label IDs back to label names\n",
    "#    - Computes seqeval metrics (precision, recall, f1, accuracy)\n",
    "#    - Returns results as a dictionary\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # TODO: Implement metric computation\n",
    "    # Hint: Use np.argmax, filter -100 labels, use seqeval.compute()\n",
    "\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, lab in zip(predictions, labels):\n",
    "        pred_labels = []\n",
    "        true_lab = []\n",
    "\n",
    "        for p_id, l_id in zip(pred, lab):\n",
    "            if l_id != -100:  \n",
    "                pred_labels.append(id2label[p_id])\n",
    "                true_lab.append(id2label[l_id])\n",
    "\n",
    "        true_predictions.append(pred_labels)\n",
    "        true_labels.append(true_lab)\n",
    "\n",
    "    results = seqeval.compute( predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41e6e403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5968f68ac2c4d76b0301ff905cc9c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-1187262438.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=args, train_dataset=datasets.Dataset.from_list(tokenized_train), eval_dataset=datasets.Dataset.from_list(tokenized_test),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260117_134136-3ww9dxua</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='801' max='801' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [801/801 04:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.157914</td>\n",
       "      <td>0.823722</td>\n",
       "      <td>0.768928</td>\n",
       "      <td>0.795383</td>\n",
       "      <td>0.961267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.140585</td>\n",
       "      <td>0.812337</td>\n",
       "      <td>0.818381</td>\n",
       "      <td>0.815348</td>\n",
       "      <td>0.969612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113800</td>\n",
       "      <td>0.145919</td>\n",
       "      <td>0.825144</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.820423</td>\n",
       "      <td>0.970281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=801, training_loss=0.0858816999323508, metrics={'train_runtime': 280.8078, 'train_samples_per_second': 45.533, 'train_steps_per_second': 2.852, 'total_flos': 592339052900340.0, 'train_loss': 0.0858816999323508, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Load the model and configure training\n",
    "# Steps:\n",
    "# 1. Import AutoModelForTokenClassification, TrainingArguments, Trainer, and DataCollatorForTokenClassification\n",
    "# 2. Load the model using AutoModelForTokenClassification.from_pretrained() with:\n",
    "#    - model_checkpoint\n",
    "#    - num_labels based on label_list length\n",
    "#    - id2label and label2id mappings\n",
    "# 3. Create TrainingArguments with:\n",
    "#    - output directory \"arabert-ner\"\n",
    "#    - evaluation_strategy=\"epoch\"\n",
    "#    - learning_rate=2e-5\n",
    "#    - batch_size=16 (both train and eval)\n",
    "#    - num_train_epochs=3\n",
    "#    - weight_decay=0.01\n",
    "# 4. Create a DataCollatorForTokenClassification for dynamic padding\n",
    "# 5. Initialize the Trainer with model, args, datasets, data_collator, tokenizer, and compute_metrics\n",
    "# 6. Call trainer.train() to start training\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"arabert-ner\", eval_strategy=\"epoch\", learning_rate=2e-5, per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16, num_train_epochs=3, weight_decay=0.01)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=datasets.Dataset.from_list(tokenized_train), eval_dataset=datasets.Dataset.from_list(tokenized_test),\n",
    "    data_collator=data_collator, tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "496b7ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: أبل, Label: ORG, Score: 0.97\n",
      "Entity: تيم كوك, Label: PERS, Score: 0.99\n",
      "Entity: الرياض, Label: LOC, Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the trained model with inference\n",
    "# Steps:\n",
    "# 1. Import pipeline from transformers\n",
    "# 2. Create an NER pipeline using the trained model and tokenizer\n",
    "# 3. Use aggregation_strategy=\"simple\" to merge sub-tokens back into words\n",
    "# 4. Test the pipeline with an Arabic text sample\n",
    "# 5. Pretty print the results showing entity, label, and confidence score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# YOUR CODE HERE\n",
    "ner_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"أعلن المدير التنفيذي لشركة أبل تيم كوك عن افتتاح فرع جديد في الرياض.\"\n",
    "results = ner_pipeline(text)\n",
    "\n",
    "# Pretty print results\n",
    "for entity in results:\n",
    "    print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
